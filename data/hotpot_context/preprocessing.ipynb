{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess hotpotqa for BERT\n",
    "- Input to BERT will be question and a paragraph\n",
    "- All sentences of the paragraph will be concatenated (possibly separated by [SEP])\n",
    "- Labels will be binary vector\n",
    "- The indices of the supporting fact vectors to extract will be supplied as a binary matrix. Multiplying by this matrix will extract the required vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "if(TRAINING):\n",
    "    file_path = \"/home/bhargav/data/hotpotqa/hotpot_train_v1.json\"\n",
    "    out_pkl_name = \"preprocessed_train.pkl\"\n",
    "    small_out_pkl_name = \"preprocessed_train_small.pkl\"\n",
    "    small_dataset_size = 5000\n",
    "    problem_indices = [8437, 25197, 34122, 46031, 52955, 63867, 82250]\n",
    "else:\n",
    "    file_path = \"/home/bhargav/data/hotpotqa/hotpot_dev_distractor_v1.json\"\n",
    "    out_pkl_name = \"preprocessed_dev\"\n",
    "    small_out_pkl_name = \"preprocessed_dev_small.pkl\"\n",
    "    small_dataset_size = 500\n",
    "    problem_indices = [5059]\n",
    "    \n",
    "    \n",
    "\n",
    "# max_seq_len = 501  # Final sequence length will be max_seq_len + (max_num_paragraphs - 1) = 510\n",
    "max_seq_len = 500\n",
    "max_sentences = 5 \n",
    "max_num_paragraphs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [03:51<00:00, 31.98it/s]\n"
     ]
    }
   ],
   "source": [
    "question_ids = []\n",
    "questions = []\n",
    "paragraphs = [] \n",
    "supporting_facts = []\n",
    "\n",
    "skipped = []\n",
    "\n",
    "for item_index, item in enumerate(tqdm(dataset)):\n",
    "    if(item_index in problem_indices):\n",
    "        skipped.append(item_index)\n",
    "        continue\n",
    "    if(len(item[\"context\"]) != 10):\n",
    "        skipped.append(item_index)\n",
    "        continue\n",
    "    question_ids.append(item[\"_id\"])\n",
    "    question = tokenize(item[\"question\"])\n",
    "    questions.append(question)\n",
    "    paragraph_names = []\n",
    "    paragraph_text = []\n",
    "    for i,para in enumerate(item[\"context\"]):\n",
    "        para_name = para[0]\n",
    "        para_sents = para[1]\n",
    "        paragraph_names.append(para_name)\n",
    "        para_sents[0] = para_name + \". \" +para_sents[0]\n",
    "        paragraph_text.append([tokenize(s) for s in para_sents])\n",
    "    paragraphs.append(paragraph_text)\n",
    "    supp_fact_list = []\n",
    "    for sup_fact in item[\"supporting_facts\"]:\n",
    "        para_name = sup_fact[0]\n",
    "        supporting_fact_index = sup_fact[1] \n",
    "        para_index = paragraph_names.index(para_name)\n",
    "        supp_fact_list.append([para_index, supporting_fact_index])\n",
    "    supporting_facts.append(supp_fact_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 61 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Skipped {} records\".format(len(skipped)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Merge all sentences in a paragraph\n",
    "- Merge question with the above and add [CLS]\n",
    "- Pad all sequences to fixed length (512 is BERT's limit)\n",
    "- Fix the number of sentences in each para\n",
    "- form the supporting fact indicator vector\n",
    "- form the matrix using which the start and end vector of each sentence can be extracted. Use the index of [PAD] for making sure each para has the same amount of sentences\n",
    "- write pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_index = tokenizer.convert_tokens_to_ids([\"[CLS]\"])[0]\n",
    "sep_index = tokenizer.convert_tokens_to_ids([\"[SEP]\"])[0]\n",
    "pad_index = tokenizer.convert_tokens_to_ids([\"[PAD]\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_paragraph(paragraph, max_seq_len):\n",
    "    assert(max_seq_len >= 0)\n",
    "    sent_lengths = [len(s) for s in paragraph]\n",
    "    out_paragraph = []\n",
    "    length_so_far = 0\n",
    "    for sent in paragraph:\n",
    "        if(len(sent) == 0):\n",
    "            continue\n",
    "        if(length_so_far + len(sent) <= max_seq_len):\n",
    "            out_paragraph.append(sent)\n",
    "            if(length_so_far + len(sent) == max_seq_len):\n",
    "                break\n",
    "            length_so_far += len(sent)      \n",
    "        else:\n",
    "            sent = sent[:max_seq_len-length_so_far]\n",
    "            out_paragraph.append(sent)\n",
    "            break\n",
    "    return out_paragraph\n",
    "    \n",
    "def pad_paragraph(paragraph, max_sequence_len, pad_index):\n",
    "    assert(max_sequence_len >= 0)\n",
    "    sent_lengths = [len(s) for s in paragraph]\n",
    "    assert(sum(sent_lengths) <= max_sequence_len)\n",
    "    paragraph[-1] += [pad_index] * (max_sequence_len - sum(sent_lengths))\n",
    "    return paragraph\n",
    "\n",
    "def merge_trim_pad_paragraphs(question, paragraph, paragraph_index, supporting_facts_in, max_seq_len, max_sentences, \n",
    "                   cls_index, sep_index, pad_index):\n",
    "    sentence_start_indices = []\n",
    "    sentence_end_indices = []\n",
    "    \n",
    "    paragraph = paragraph[:max_sentences]\n",
    "    \n",
    "    total_para_len_words = sum([len(s) for s in paragraph])\n",
    "    \n",
    "    available_length_for_paragraph = max_seq_len - (len(question) + 2) # question + CLS + SEP\n",
    "    \n",
    "    if(total_para_len_words >= available_length_for_paragraph):\n",
    "        paragraph = trim_paragraph(paragraph, available_length_for_paragraph-1) #-1 to make room for the next empty sentence\n",
    "        paragraph[-1].append(pad_index)\n",
    "    elif(total_para_len_words < available_length_for_paragraph):\n",
    "        paragraph = pad_paragraph(paragraph, available_length_for_paragraph, pad_index)\n",
    "        \n",
    "        \n",
    "    #concatenate sentences, note starting and ending indices of sentences\n",
    "    sentence_start_indices = []\n",
    "    sentence_end_indices = []\n",
    "    out_sequence = [cls_index] + question + [sep_index]\n",
    "    for sent in paragraph:\n",
    "        sentence_start_indices.append(len(out_sequence))\n",
    "        out_sequence += sent\n",
    "        sentence_end_indices.append(len(out_sequence)-1)\n",
    "    \n",
    "    assert(len(sentence_start_indices) == len(sentence_end_indices))\n",
    "    \n",
    "    #create the matrix used to extract first and last vectors of sentences\n",
    "#     start_index_matrix = []\n",
    "#     end_index_matrix = []\n",
    "#     for i in range(len(sentence_start_indices)):\n",
    "#         start_indicator_vector = [0] * max_seq_len\n",
    "#         end_indicator_vector = [0] * max_seq_len\n",
    "#         start_indicator_vector[sentence_start_indices[i]] = 1\n",
    "#         end_indicator_vector[sentence_end_indices[i]] = 1\n",
    "#         start_index_matrix.append(start_indicator_vector)\n",
    "#         end_index_matrix.append(end_indicator_vector)\n",
    "        \n",
    "    #create supporting_facts vector\n",
    "    supporting_facts = [0] * max_sentences\n",
    "    for s_f in supporting_facts_in:\n",
    "        if(s_f[0] == paragraph_index and s_f[1]<max_sentences):\n",
    "            supporting_facts[s_f[1]] = 1\n",
    "            \n",
    "    #expand start and end index matrices to make sure all extract equal number of vectors (=max_sentences)\n",
    "#     if(max_sentences - len(paragraph) > 0):\n",
    "#         fake_sentence_start_and_end = [0] * max_seq_len\n",
    "#         index_of_pad = out_sequence.index(pad_index)\n",
    "#         fake_sentence_start_and_end[index_of_pad] = 1\n",
    "#         for i in range(max_sentences - len(paragraph)):\n",
    "#             start_index_matrix.append(fake_sentence_start_and_end)\n",
    "#             end_index_matrix.append(fake_sentence_start_and_end)\n",
    "            \n",
    "    segment_id = [0]*(len(question) + 2)\n",
    "    segment_id += [1]*(max_seq_len - (len(question) + 2))\n",
    "    \n",
    "    # sanity check\n",
    "    assert(len(out_sequence) == max_seq_len)\n",
    "    assert(len(segment_id) == max_seq_len)\n",
    "#     assert(len(start_index_matrix) == max_sentences)\n",
    "#     assert(len(end_index_matrix) == max_sentences)\n",
    "    assert(len(supporting_facts) == max_sentences)\n",
    "    \n",
    "    return {'sequence': out_sequence, #'start_index':start_index_matrix, 'end_index':end_index_matrix, \n",
    "            'sentence_start_index': sentence_start_indices, 'sentence_end_index': sentence_end_indices,\n",
    "            'supporting_fact': supporting_facts, 'segment_id':segment_id}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7344/7344 [00:05<00:00, 1337.45it/s]\n"
     ]
    }
   ],
   "source": [
    "out_dict = {'sequence': [], 'sentence_start_index':[], 'sentence_end_index':[], 'supporting_fact': [], 'segment_id': [],\n",
    "           'max_seq_len':max_seq_len, 'max_sentences':max_sentences, \"question_id\":[]}\n",
    "q_count = 0\n",
    "p_count = 0\n",
    "for i,q in enumerate(tqdm(questions)):\n",
    "    q_count+=1\n",
    "    for j,para in enumerate(paragraphs[i]):\n",
    "        p_count += 1\n",
    "        processed_example = merge_trim_pad_paragraphs(question=q, paragraph=para, paragraph_index=j, \n",
    "                                                      supporting_facts_in=supporting_facts[i], \n",
    "                                                      max_seq_len=max_seq_len, max_sentences=max_sentences, \n",
    "                                                      cls_index=cls_index, sep_index=sep_index, pad_index=pad_index)\n",
    "        out_dict[\"question_id\"].append(question_ids[i])\n",
    "        for key,value in processed_example.items():\n",
    "            out_dict[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({10: 7344})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([len(p) for p in paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7344\n",
      "73440\n"
     ]
    }
   ],
   "source": [
    "print(q_count)\n",
    "print(p_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save number of sentences per document and number of paragraphs per document so that the predictions can be \n",
    "# reformatted to the standard format\n",
    "document_lengths = []\n",
    "\n",
    "for doc in paragraphs:\n",
    "    num_sentences = []\n",
    "    for para in doc:\n",
    "        num_sentences.append(len(para))\n",
    "    document_lengths.append(num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict['document_length'] = document_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:sequence, value_length:73440\n",
      "key:sentence_start_index, value_length:73440\n",
      "key:sentence_end_index, value_length:73440\n",
      "key:supporting_fact, value_length:73440\n",
      "key:segment_id, value_length:73440\n",
      "key:question_id, value_length:73440\n",
      "key:document_length, value_length:7344\n"
     ]
    }
   ],
   "source": [
    "for key,value in out_dict.items():\n",
    "    if(type(value) == list):\n",
    "        print(\"key:{}, value_length:{}\".format(key, len(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{500}\n"
     ]
    }
   ],
   "source": [
    "lengths = set([len(s) for s in out_dict[\"sequence\"]])\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{500}\n"
     ]
    }
   ],
   "source": [
    "lengths = set([len(s) for s in out_dict[\"segment_id\"]])\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 50, 85]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict[\"sentence_start_index\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7344"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73440"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_dict[\"question_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73440/73440 [00:00<00:00, 184079.65it/s]\n"
     ]
    }
   ],
   "source": [
    "the_real_out_dict = {}\n",
    "for i in range(max_num_paragraphs):\n",
    "    the_real_out_dict[\"sequence_{}\".format(i)] = []\n",
    "    the_real_out_dict[\"sentence_start_index_{}\".format(i)] = []\n",
    "    the_real_out_dict[\"sentence_end_index_{}\".format(i)] = []\n",
    "    the_real_out_dict[\"supporting_fact_{}\".format(i)] = []\n",
    "    the_real_out_dict[\"segment_id_{}\".format(i)] = []\n",
    "\n",
    "the_real_out_dict[\"document_length\"] = out_dict['document_length']\n",
    "\n",
    "for i in trange(len(out_dict[\"sequence\"])):\n",
    "#     the_real_out_dict[\"sequence_{}\".format(i%10)].append(out_dict[\"sequence\"][i] + [0]*(max_num_paragraphs-1))\n",
    "    the_real_out_dict[\"sequence_{}\".format(i%10)].append(out_dict[\"sequence\"][i])\n",
    "    the_real_out_dict[\"sentence_start_index_{}\".format(i%10)].append(out_dict[\"sentence_start_index\"][i])\n",
    "    the_real_out_dict[\"sentence_end_index_{}\".format(i%10)].append(out_dict[\"sentence_end_index\"][i])\n",
    "    the_real_out_dict[\"supporting_fact_{}\".format(i%10)].append(out_dict[\"supporting_fact\"][i])\n",
    "#     the_real_out_dict[\"segment_id_{}\".format(i%10)].append(out_dict[\"segment_id\"][i] + [1]*(max_num_paragraphs-1))\n",
    "    the_real_out_dict[\"segment_id_{}\".format(i%10)].append(out_dict[\"segment_id\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:sequence_0, value_length:7344\n",
      "key:sentence_start_index_0, value_length:7344\n",
      "key:sentence_end_index_0, value_length:7344\n",
      "key:supporting_fact_0, value_length:7344\n",
      "key:segment_id_0, value_length:7344\n",
      "key:sequence_1, value_length:7344\n",
      "key:sentence_start_index_1, value_length:7344\n",
      "key:sentence_end_index_1, value_length:7344\n",
      "key:supporting_fact_1, value_length:7344\n",
      "key:segment_id_1, value_length:7344\n",
      "key:sequence_2, value_length:7344\n",
      "key:sentence_start_index_2, value_length:7344\n",
      "key:sentence_end_index_2, value_length:7344\n",
      "key:supporting_fact_2, value_length:7344\n",
      "key:segment_id_2, value_length:7344\n",
      "key:sequence_3, value_length:7344\n",
      "key:sentence_start_index_3, value_length:7344\n",
      "key:sentence_end_index_3, value_length:7344\n",
      "key:supporting_fact_3, value_length:7344\n",
      "key:segment_id_3, value_length:7344\n",
      "key:sequence_4, value_length:7344\n",
      "key:sentence_start_index_4, value_length:7344\n",
      "key:sentence_end_index_4, value_length:7344\n",
      "key:supporting_fact_4, value_length:7344\n",
      "key:segment_id_4, value_length:7344\n",
      "key:sequence_5, value_length:7344\n",
      "key:sentence_start_index_5, value_length:7344\n",
      "key:sentence_end_index_5, value_length:7344\n",
      "key:supporting_fact_5, value_length:7344\n",
      "key:segment_id_5, value_length:7344\n",
      "key:sequence_6, value_length:7344\n",
      "key:sentence_start_index_6, value_length:7344\n",
      "key:sentence_end_index_6, value_length:7344\n",
      "key:supporting_fact_6, value_length:7344\n",
      "key:segment_id_6, value_length:7344\n",
      "key:sequence_7, value_length:7344\n",
      "key:sentence_start_index_7, value_length:7344\n",
      "key:sentence_end_index_7, value_length:7344\n",
      "key:supporting_fact_7, value_length:7344\n",
      "key:segment_id_7, value_length:7344\n",
      "key:sequence_8, value_length:7344\n",
      "key:sentence_start_index_8, value_length:7344\n",
      "key:sentence_end_index_8, value_length:7344\n",
      "key:supporting_fact_8, value_length:7344\n",
      "key:segment_id_8, value_length:7344\n",
      "key:sequence_9, value_length:7344\n",
      "key:sentence_start_index_9, value_length:7344\n",
      "key:sentence_end_index_9, value_length:7344\n",
      "key:supporting_fact_9, value_length:7344\n",
      "key:segment_id_9, value_length:7344\n",
      "key:document_length, value_length:7344\n"
     ]
    }
   ],
   "source": [
    "for key,value in the_real_out_dict.items():\n",
    "    if(type(value) == list):\n",
    "        print(\"key:{}, value_length:{}\".format(key, len(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(the_real_out_dict[\"sequence_0\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(the_real_out_dict[\"segment_id_0\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_out_dict = {}\n",
    "for key, value in the_real_out_dict.items():\n",
    "    small_out_dict[key] = value[:small_dataset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:sequence_0, value_length:500\n",
      "key:sentence_start_index_0, value_length:500\n",
      "key:sentence_end_index_0, value_length:500\n",
      "key:supporting_fact_0, value_length:500\n",
      "key:segment_id_0, value_length:500\n",
      "key:sequence_1, value_length:500\n",
      "key:sentence_start_index_1, value_length:500\n",
      "key:sentence_end_index_1, value_length:500\n",
      "key:supporting_fact_1, value_length:500\n",
      "key:segment_id_1, value_length:500\n",
      "key:sequence_2, value_length:500\n",
      "key:sentence_start_index_2, value_length:500\n",
      "key:sentence_end_index_2, value_length:500\n",
      "key:supporting_fact_2, value_length:500\n",
      "key:segment_id_2, value_length:500\n",
      "key:sequence_3, value_length:500\n",
      "key:sentence_start_index_3, value_length:500\n",
      "key:sentence_end_index_3, value_length:500\n",
      "key:supporting_fact_3, value_length:500\n",
      "key:segment_id_3, value_length:500\n",
      "key:sequence_4, value_length:500\n",
      "key:sentence_start_index_4, value_length:500\n",
      "key:sentence_end_index_4, value_length:500\n",
      "key:supporting_fact_4, value_length:500\n",
      "key:segment_id_4, value_length:500\n",
      "key:sequence_5, value_length:500\n",
      "key:sentence_start_index_5, value_length:500\n",
      "key:sentence_end_index_5, value_length:500\n",
      "key:supporting_fact_5, value_length:500\n",
      "key:segment_id_5, value_length:500\n",
      "key:sequence_6, value_length:500\n",
      "key:sentence_start_index_6, value_length:500\n",
      "key:sentence_end_index_6, value_length:500\n",
      "key:supporting_fact_6, value_length:500\n",
      "key:segment_id_6, value_length:500\n",
      "key:sequence_7, value_length:500\n",
      "key:sentence_start_index_7, value_length:500\n",
      "key:sentence_end_index_7, value_length:500\n",
      "key:supporting_fact_7, value_length:500\n",
      "key:segment_id_7, value_length:500\n",
      "key:sequence_8, value_length:500\n",
      "key:sentence_start_index_8, value_length:500\n",
      "key:sentence_end_index_8, value_length:500\n",
      "key:supporting_fact_8, value_length:500\n",
      "key:segment_id_8, value_length:500\n",
      "key:sequence_9, value_length:500\n",
      "key:sentence_start_index_9, value_length:500\n",
      "key:sentence_end_index_9, value_length:500\n",
      "key:supporting_fact_9, value_length:500\n",
      "key:segment_id_9, value_length:500\n",
      "key:document_length, value_length:500\n"
     ]
    }
   ],
   "source": [
    "for key,value in small_out_dict.items():\n",
    "    if(type(value) == list):\n",
    "        print(\"key:{}, value_length:{}\".format(key, len(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickler(out_pkl_path, small_out_pkl_name, small_out_dict)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path, out_pkl_name, the_real_out_dict)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
