{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import dill as pickle\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting \n",
    "\n",
    "Each data point will be [question_id, question, [context sentences], [supporting fact indicators]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "\n",
    "if TRAIN:\n",
    "    dataset_file_path = \"/home/bhargav/data/squad/train-v1.1.json\"\n",
    "    out_pkl_name = \"preprocessed_train.pkl\"\n",
    "else:\n",
    "    dataset_file_path = \"/home/bhargav/data/squad/dev-v1.1.json\"\n",
    "    out_pkl_name = \"preprocessed_dev.pkl\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_has_answer(sent,ans):\n",
    "    sent_tok = set(nltk.word_tokenize(sent))\n",
    "    ans_tok = set(nltk.word_tokenize(ans))\n",
    "    return len(ans_tok.difference(sent_tok)) == 0\n",
    "\n",
    "def put_space_before_period(sents):\n",
    "    output = []\n",
    "    for sent in sents:\n",
    "        s = nltk.word_tokenize(sent)\n",
    "        output.append(\" \".join(s))\n",
    "    return output\n",
    "\n",
    "# Input: number of characters in each sentence, start pointer of the answer(given in the dataset)\n",
    "# Output: one hot vector indicating the sentence containing the answer\n",
    "def find_answer_sentence(sentence_lengths, answer_start):\n",
    "    length_so_far = 0\n",
    "    sentence_index = 0\n",
    "    for i, length in enumerate(sentence_lengths):\n",
    "        if(length_so_far <= answer_start <= length_so_far+length):\n",
    "            sentence_index = i\n",
    "            break\n",
    "        else:\n",
    "            length_so_far += length\n",
    "    out_vector = [0 for i in sentence_lengths]\n",
    "    out_vector[sentence_index] = 1\n",
    "    return out_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(file_path, encoding='utf8') as file:\n",
    "#     dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "#     exclude = set(string.punctuation)\n",
    "#     clean = ''.join(ch for ch in text if ch not in exclude)\n",
    "#     clean = clean.lower().strip()\n",
    "    text = re.sub(\n",
    "            r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "            str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(dataset):\n",
    "    ids = []\n",
    "    questions = []\n",
    "    sentences = []\n",
    "    answers = []\n",
    "    for article in tqdm(dataset):\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context_raw = paragraph['context']\n",
    "            context_sents = nltk.sent_tokenize(context_raw)\n",
    "            # +1 to count the spaces after the '.' after each sentence\n",
    "            context_sent_lengths = [len(x)+1 for x in context_sents]\n",
    "            for qa in paragraph['qas']:\n",
    "                gt_start_pointers = list(map(lambda x: x['answer_start'], qa['answers']))\n",
    "                answer_indicators = find_answer_sentence(sentence_lengths=context_sent_lengths, answer_start=gt_start_pointers[0])                \n",
    "                sentences.append(context_sents)\n",
    "                questions.append(qa['question'])\n",
    "                ids.append(qa['id'])\n",
    "                answers.append(answer_indicators)\n",
    "    return [ids, questions, sentences, answers]\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataset_file_path):\n",
    "    with open(dataset_file_path) as dataset_file:\n",
    "        dataset_json = json.load(dataset_file)\n",
    "        dataset = dataset_json['data']\n",
    "    records = format_dataset(dataset)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:01<00:00, 44.14it/s]\n"
     ]
    }
   ],
   "source": [
    "ids, questions, sentences, supporting_facts = run(dataset_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570\n",
      "10570\n",
      "10570\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))\n",
    "print(len(sentences))\n",
    "print(len(supporting_facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which NFL team represented the AFC at Super Bowl 50?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.',\n",
       " 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.',\n",
       " \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n",
       " 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supporting_facts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_with_id(dataset, idx = \"5ae61bfd5542992663a4f261\"):\n",
    "    index = -1\n",
    "    for i,item in enumerate(dataset):\n",
    "        if(item[\"_id\"] == idx):\n",
    "            return i\n",
    "    return index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formatted_example(index, question_ids, questions, sentences, \n",
    "                            supporting_facts):\n",
    "    separator = \"--xx--xx--xx--xx--xx--xx--\"\n",
    "    print(\"Question id:\",question_ids[index])\n",
    "    print(separator)\n",
    "    print(\"Question:\",questions[index])\n",
    "    print(separator)\n",
    "    print(\"sentences:\")\n",
    "    for i, sent in enumerate(sentences[index]):\n",
    "        print(\"{} :{}\".format(i,sent))\n",
    "    print(separator)\n",
    "    print(\"supporting_facts:\",supporting_facts[index])\n",
    "    print(separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question id: 56bea9923aeaaa14008c91bb\n",
      "--xx--xx--xx--xx--xx--xx--\n",
      "Question: What day was the Super Bowl played on?\n",
      "--xx--xx--xx--xx--xx--xx--\n",
      "sentences:\n",
      "0 :Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.\n",
      "1 :The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.\n",
      "2 :The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\n",
      "3 :As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "--xx--xx--xx--xx--xx--xx--\n",
      "supporting_facts: [0, 0, 1, 0]\n",
      "--xx--xx--xx--xx--xx--xx--\n"
     ]
    }
   ],
   "source": [
    "print_formatted_example(10, ids, questions, sentences, \n",
    "                            supporting_facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '!', 'i', 'don', \"'\", 't', 'like', 'you', ':', '-', ')', '!', '!', '!', '!', '(', 'yo', '##lo', ')']\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "    tokens = tokenizer.tokenize(\"     hello, world! I don't like you :-)  !!!! (YOLO)\")\n",
    "    print(tokens)\n",
    "\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tokenize(text_in):\n",
    "    text_out = []\n",
    "    for line in text_in:\n",
    "        tokens = tokenize(line)\n",
    "        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        text_out.append(tokens)\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19204, 4697, 2023], [1998, 2023, 999]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokenize([\"tokenize this\",\"and this!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_tokenized = batch_tokenize(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2029, 5088, 2136, 3421, 1996, 10511, 2012, 3565, 4605, 2753, 1029]\n"
     ]
    }
   ],
   "source": [
    "print(questions_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [00:49<00:00, 213.45it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences_tokenized = []\n",
    "for sent_list in tqdm(sentences):\n",
    "    sents_tokenized = batch_tokenize(sent_list)\n",
    "    sentences_tokenized.append(sents_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3565, 4605, 2753, 2001, 2019, 2137, 2374, 2208, 2000, 5646, 1996, 3410, 1997, 1996, 2120, 2374, 2223, 1006, 5088, 1007, 2005, 1996, 2325, 2161, 1012], [1996, 2137, 2374, 3034, 1006, 10511, 1007, 3410, 7573, 14169, 3249, 1996, 2120, 2374, 3034, 1006, 22309, 1007, 3410, 3792, 12915, 2484, 1516, 2184, 2000, 7796, 2037, 2353, 3565, 4605, 2516, 1012], [1996, 2208, 2001, 2209, 2006, 2337, 1021, 1010, 2355, 1010, 2012, 11902, 1005, 1055, 3346, 1999, 1996, 2624, 3799, 3016, 2181, 2012, 4203, 10254, 1010, 2662, 1012], [2004, 2023, 2001, 1996, 12951, 3565, 4605, 1010, 1996, 2223, 13155, 1996, 1000, 3585, 5315, 1000, 2007, 2536, 2751, 1011, 11773, 11107, 1010, 2004, 2092, 2004, 8184, 28324, 2075, 1996, 4535, 1997, 10324, 2169, 3565, 4605, 2208, 2007, 3142, 16371, 28990, 2015, 1006, 2104, 2029, 1996, 2208, 2052, 2031, 2042, 2124, 2004, 1000, 3565, 4605, 1048, 1000, 1007, 1010, 2061, 2008, 1996, 8154, 2071, 14500, 3444, 1996, 5640, 16371, 28990, 2015, 2753, 1012]]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_plus_sentence_len = 103\n",
    "max_sentences_per_passage = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_index = tokenizer.convert_tokens_to_ids([\"[CLS]\"])[0]\n",
    "sep_index = tokenizer.convert_tokens_to_ids([\"[SEP]\"])[0]\n",
    "pad_index = tokenizer.convert_tokens_to_ids([\"[PAD]\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_trim_pad(sent_1, sent_2, max_len, cls_index, sep_index, pad_index):\n",
    "    merged_seq = [cls_index] + sent_1 + [sep_index] + sent_2\n",
    "    merged_seq = merged_seq[:max_len-1]\n",
    "    merged_seq.append(sep_index)\n",
    "    merged_seq += [pad_index] * (max_len - len(merged_seq))\n",
    "    num_zeros = len(sent_1) + 2\n",
    "    segment_id = [0]*num_zeros + [1]*(len(merged_seq)-num_zeros)\n",
    "    mask  = []\n",
    "    for id in merged_seq:\n",
    "        if(id == pad_index):\n",
    "            mask.append(0)\n",
    "        else:\n",
    "            mask.append(1)\n",
    "    return merged_seq, segment_id, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]', 1, 1, '[SEP]', 2, 2, 2, '[SEP]', ['PAD'], ['PAD']],\n",
       " [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_trim_pad([1,1], [2,2,2], 10, \"[CLS]\", \"[SEP]\", [\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]', 1, 1, '[SEP]', '[SEP]'], [0, 0, 0, 0, 1], [1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_trim_pad([1,1], [2,2,2], 5, \"[CLS]\", \"[SEP]\", [\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]', 1, 1, '[SEP]', 2, 2, 2, '[SEP]'],\n",
       " [0, 0, 0, 0, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_trim_pad([1,1], [2,2,2], 8, \"[CLS]\", \"[SEP]\", [\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]', '[SEP]', '[SEP]', ['PAD'], ['PAD']],\n",
       " [0, 0, 1, 1, 1],\n",
       " [1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_trim_pad([], [], 5, \"[CLS]\", \"[SEP]\", [\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 1, 1, 102, 2, 2, 2, 102, 0, 0],\n",
       " [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_trim_pad([1,1], [2,2,2], 10, cls_index, sep_index, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [00:03<00:00, 2682.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# init data dict\n",
    "data_out = {}\n",
    "\n",
    "for i in range(max_sentences_per_passage):\n",
    "    data_out[\"sequence_{}\".format(i)] = []\n",
    "    data_out[\"sequence_segment_id_{}\".format(i)] = []\n",
    "    #data_out[\"sequence_mask_{}\".format(i)] = []\n",
    "\n",
    "data_out[\"passage_mask\"] = []\n",
    "data_out[\"supporting_fact\"] = []\n",
    "\n",
    "\n",
    "for i in trange(len(questions_tokenized)):\n",
    "    question = questions_tokenized[i]\n",
    "    sentences = sentences_tokenized[i][:max_sentences_per_passage]\n",
    "    num_pad_sentences = max_sentences_per_passage - len(sentences)\n",
    "    sentences = sentences + [[]]*(num_pad_sentences)\n",
    "    \n",
    "    passage_mask = [1] * (max_sentences_per_passage-num_pad_sentences) + [0]*num_pad_sentences\n",
    "    \n",
    "    \n",
    "    supp_fact = supporting_facts[i]\n",
    "    \n",
    "    supp_fact = supp_fact[:max_sentences_per_passage]\n",
    "    \n",
    "    # Dont append anything to data_out before this\n",
    "    # Skip the training questions who's passage loses supporting fact due to trimming\n",
    "    if(TRAIN):\n",
    "        if(sum(supp_fact) == 0):\n",
    "            continue\n",
    "            \n",
    "    data_out[\"passage_mask\"].append(passage_mask)\n",
    "    \n",
    "    supp_fact = supp_fact + [0]*(num_pad_sentences)\n",
    "    data_out[\"supporting_fact\"].append(supp_fact)\n",
    "    \n",
    "    for j,sent in enumerate(sentences):\n",
    "        merged_seq, segment_id, mask = merge_trim_pad(sent_1=question, sent_2=sent, \n",
    "                                    max_len=max_question_plus_sentence_len, \n",
    "                                    cls_index=cls_index, sep_index=sep_index, pad_index=pad_index)\n",
    "        data_out[\"sequence_{}\".format(j)].append(merged_seq)\n",
    "        data_out[\"sequence_segment_id_{}\".format(j)].append(segment_id)\n",
    "        #data_out[\"sequence_mask_{}\".format(j)].append(mask)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sequence_0', 'sequence_segment_id_0', 'sequence_1', 'sequence_segment_id_1', 'sequence_2', 'sequence_segment_id_2', 'sequence_3', 'sequence_segment_id_3', 'sequence_4', 'sequence_segment_id_4', 'sequence_5', 'sequence_segment_id_5', 'sequence_6', 'sequence_segment_id_6', 'sequence_7', 'sequence_segment_id_7', 'sequence_8', 'sequence_segment_id_8', 'sequence_9', 'sequence_segment_id_9', 'passage_mask', 'supporting_fact'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2029, 5088, 2136, 3421, 1996, 10511, 2012, 3565, 4605, 2753, 1029, 102, 3565, 4605, 2753, 2001, 2019, 2137, 2374, 2208, 2000, 5646, 1996, 3410, 1997, 1996, 2120, 2374, 2223, 1006, 5088, 1007, 2005, 1996, 2325, 2161, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(data_out['sequence_0'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(data_out['sequence_segment_id_0'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_out['sequence_mask_0'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_out['sequence_0'][0].count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_out['sequence_mask_0'][0].count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lengths = []\n",
    "both = []\n",
    "for key,value in data_out.items():\n",
    "    both.append([key, len(value)])\n",
    "    all_lengths.append(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570\n",
      "10570\n"
     ]
    }
   ],
   "source": [
    "print(min(all_lengths))\n",
    "print(max(all_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sequence_0', 10570],\n",
       " ['sequence_segment_id_0', 10570],\n",
       " ['sequence_1', 10570],\n",
       " ['sequence_segment_id_1', 10570],\n",
       " ['sequence_2', 10570],\n",
       " ['sequence_segment_id_2', 10570],\n",
       " ['sequence_3', 10570],\n",
       " ['sequence_segment_id_3', 10570],\n",
       " ['sequence_4', 10570],\n",
       " ['sequence_segment_id_4', 10570],\n",
       " ['sequence_5', 10570],\n",
       " ['sequence_segment_id_5', 10570],\n",
       " ['sequence_6', 10570],\n",
       " ['sequence_segment_id_6', 10570],\n",
       " ['sequence_7', 10570],\n",
       " ['sequence_segment_id_7', 10570],\n",
       " ['sequence_8', 10570],\n",
       " ['sequence_segment_id_8', 10570],\n",
       " ['sequence_9', 10570],\n",
       " ['sequence_segment_id_9', 10570],\n",
       " ['passage_mask', 10570],\n",
       " ['supporting_fact', 10570]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path,out_pkl_name,data_out)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
