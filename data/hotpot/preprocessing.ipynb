{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing HotpotQA for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "if(TRAINING):\n",
    "    file_path = \"/home/bhargav/data/hotpotqa/hotpot_train_v1.json\"\n",
    "    out_pkl_name = \"preprocessed_train.pkl\"\n",
    "    problem_indices = [8437, 25197, 34122, 46031, 52955, 63867, 82250]\n",
    "else:\n",
    "    file_path = \"/home/bhargav/data/hotpotqa/hotpot_dev_distractor_v1.json\"\n",
    "    out_pkl_name = \"preprocessed_dev.pkl\"\n",
    "    problem_indices = [5059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "#     exclude = set(string.punctuation)\n",
    "#     clean = ''.join(ch for ch in text if ch not in exclude)\n",
    "#     clean = clean.lower().strip()\n",
    "    text = re.sub(\n",
    "            r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "            str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90564/90564 [45:26<00:00, 32.53it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "paragraphs = [] \n",
    "supporting_facts = []\n",
    "\n",
    "\n",
    "for item_index, item in enumerate(tqdm(dataset)):\n",
    "    if(item_index in problem_indices):\n",
    "        continue\n",
    "    question = tokenize(item[\"question\"])\n",
    "    questions.append(question)\n",
    "    paragraph_names = []\n",
    "    paragraph_text = []\n",
    "    for i,para in enumerate(item[\"context\"]):\n",
    "        para_name = para[0]\n",
    "        para_sents = para[1]\n",
    "        paragraph_names.append(para_name)\n",
    "        paragraph_text.append([tokenize(s) for s in para_sents])\n",
    "    paragraphs.append(paragraph_text)\n",
    "    supp_fact_list = []\n",
    "    for sup_fact in item[\"supporting_facts\"]:\n",
    "        para_name = sup_fact[0]\n",
    "        supporting_fact_index = sup_fact[1] \n",
    "        para_index = paragraph_names.index(para_name)\n",
    "        supp_fact_list.append([para_index, supporting_fact_index])\n",
    "    supporting_facts.append(supp_fact_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2073, 2001, 1996, 2364, 2839, 1999, 1996, 2143, 22953, 15551, 14620, 1999, 2152, 1011, 3036, 13691, 8323, 1029]\n"
     ]
    }
   ],
   "source": [
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2798, 4300, 1000, 4918, 1000, 10582, 1006, 2141, 2745, 5146, 12001, 2036, 2124, 2004, 2798, 22953, 15551, 1025, 1020, 2285, 3999, 1007, 2003, 2019, 2394, 4735, 2040, 2003, 2411, 3615, 2000, 1999, 1996, 2329, 2811, 2004, 1996, 1000, 2087, 6355, 7267, 1999, 3725, 1000, 1998, 1000, 3725, 1005, 1055, 2087, 12536, 7267, 1000, 1012], [2002, 2038, 2985, 6993, 14620, 1999, 1996, 13276, 2669, 1010, 5041, 17622, 1998, 6683, 5172, 2152, 1011, 3036, 13691, 8323, 1012]]\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 1], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(supporting_facts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90557\n",
      "90557\n",
      "10\n",
      "90557\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))\n",
    "print(len(paragraphs))\n",
    "print(len(paragraphs[0]))\n",
    "print(len(supporting_facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_paragraph_lengths(document):\n",
    "    lengths = []\n",
    "    for para in document:\n",
    "        lengths.append(len(para))\n",
    "    return lengths, sum(lengths)\n",
    "\n",
    "\n",
    "# returns supporting fact indices so that it can be used later while trimming documents.\n",
    "def expand_supporting_facts(supporting_facts, paragraphs):\n",
    "    supporting_facts_expanded = []\n",
    "    problem_indices = []\n",
    "    supporting_fact_indices = []\n",
    "    for i,supp_facts in enumerate(tqdm(supporting_facts)):\n",
    "        s_f_indices = []\n",
    "        paragraph_lengths, total_num_sentences = compute_paragraph_lengths(paragraphs[i])\n",
    "        s_f_expanded = [0] * total_num_sentences\n",
    "        for para_idx, sentence_idx in supp_facts:\n",
    "            fact_idx = sum(paragraph_lengths[:para_idx])+ sentence_idx\n",
    "            if(fact_idx >= total_num_sentences):\n",
    "                problem_indices.append(i)\n",
    "            else:\n",
    "                s_f_indices.append(fact_idx)\n",
    "        for s_f_idx in s_f_indices:\n",
    "            s_f_expanded[s_f_idx] = 1.0\n",
    "        supporting_facts_expanded.append(s_f_expanded)\n",
    "        supporting_fact_indices += s_f_indices\n",
    "    return supporting_facts_expanded, set(problem_indices), s_f_indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90557/90557 [00:00<00:00, 114732.30it/s]\n"
     ]
    }
   ],
   "source": [
    "supporting_facts_expanded, problem_indices, supporting_fact_indices = expand_supporting_facts(supporting_facts, paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg supporting fact index:18.0\n",
      "min supporting fact index:3\n",
      "max supporting fact index:33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supporting_fact_indices = np.array(supporting_fact_indices)\n",
    "\n",
    "print(\"Avg supporting fact index:{}\".format(supporting_fact_indices.mean()))\n",
    "print(\"min supporting fact index:{}\".format(supporting_fact_indices.min()))\n",
    "print(\"max supporting fact index:{}\".format(supporting_fact_indices.max()))\n",
    "\n",
    "max_supporting_fact_index = 35\n",
    "np.sum(np.greater(supporting_fact_indices,max_supporting_fact_index))/supporting_fact_indices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(supporting_facts_expanded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_documents(documents):\n",
    "    flattened_documents = []\n",
    "    for doc in tqdm(documents):\n",
    "        f_d = []\n",
    "        for para in doc:\n",
    "            for sent in para:\n",
    "                f_d.append(sent)\n",
    "        flattened_documents.append(f_d)\n",
    "    return flattened_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90557/90557 [00:00<00:00, 95061.99it/s]\n"
     ]
    }
   ],
   "source": [
    "flattened_documents = flatten_documents(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(flattened_documents[0]) == len(supporting_facts_expanded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg document len:40.926985213732785\n",
      "min document len:2\n",
      "max document len:144\n"
     ]
    }
   ],
   "source": [
    "num_sentences_per_document = []\n",
    "\n",
    "for item in supporting_facts_expanded:\n",
    "    num_sentences_per_document.append(len(item))\n",
    "\n",
    "num_sentences_per_document = np.array(num_sentences_per_document)\n",
    "\n",
    "print(\"Avg document len:{}\".format(num_sentences_per_document.mean()))\n",
    "print(\"min document len:{}\".format(num_sentences_per_document.min()))\n",
    "print(\"max document len:{}\".format(num_sentences_per_document.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025884249699084556"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentences_per_document = 65\n",
    "np.sum(np.greater(num_sentences_per_document,max_sentences_per_document))/num_sentences_per_document.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg sentence len:28.858022381263954\n",
      "min sentence len:0\n",
      "max sentence len:949\n"
     ]
    }
   ],
   "source": [
    "sentence_lengths = []\n",
    "\n",
    "for doc in flattened_documents:\n",
    "    for sent in doc:\n",
    "        sentence_lengths.append(len(sent))\n",
    "\n",
    "sentence_lengths = np.array(sentence_lengths)\n",
    "\n",
    "print(\"Avg sentence len:{}\".format(sentence_lengths.mean()))\n",
    "print(\"min sentence len:{}\".format(sentence_lengths.min()))\n",
    "print(\"max sentence len:{}\".format(sentence_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05598472839614432"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_len = 55\n",
    "np.sum(np.greater(sentence_lengths,max_sent_len))/sentence_lengths.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg question len:22.418277990657817\n",
      "min question len:1\n",
      "max question len:141\n"
     ]
    }
   ],
   "source": [
    "question_lengths = np.array([len(q) for q in questions])\n",
    "\n",
    "print(\"Avg question len:{}\".format(question_lengths.mean()))\n",
    "print(\"min question len:{}\".format(question_lengths.min()))\n",
    "print(\"max question len:{}\".format(question_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05349117130646996"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len = 45\n",
    "np.sum(np.greater(question_lengths,max_question_len))/question_lengths.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_plus_sentence_len = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_index = tokenizer.convert_tokens_to_ids([\"[CLS]\"])[0]\n",
    "sep_index = tokenizer.convert_tokens_to_ids([\"[SEP]\"])[0]\n",
    "pad_index = tokenizer.convert_tokens_to_ids([\"[PAD]\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_trim_pad(sent_1, sent_2, max_len, cls_index, sep_index, pad_index):\n",
    "    merged_seq = [cls_index] + sent_1 + [sep_index] + sent_2\n",
    "    merged_seq = merged_seq[:max_len-1]\n",
    "    merged_seq.append(sep_index)\n",
    "    merged_seq = merged_seq + [pad_index] * (max_len - len(merged_seq))\n",
    "    num_zeros = merged_seq.index(sep_index) + 1 # index of [SEP] + 1 =  number of zeros to add\n",
    "    segment_id = [0]*num_zeros + [1]*(len(merged_seq)-num_zeros)\n",
    "    return merged_seq, segment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a messy function. I have wrapped it in a function because i have lifted this code from my squad sentence selector\n",
    "# and the variable names inside and outside the functions conflict.\n",
    "def do_a_lot_of_work(questions_tokenized, sentences_tokenized, supporting_facts, \n",
    "                     max_question_plus_sentence_len, cls_index, sep_index, pad_index):\n",
    "    # init data dict\n",
    "    data_out = {\"sequences\":[], \"segment_ids\":[], \"supporting_fact\":[], \"document_lengths\":[]}\n",
    "    \n",
    "    for i in trange(len(questions_tokenized)):\n",
    "        question = questions_tokenized[i]\n",
    "        sentences = sentences_tokenized[i]\n",
    "        supp_fact = supporting_facts[i]\n",
    "        \n",
    "        data_out[\"document_lengths\"].append(len(supp_fact))\n",
    "        \n",
    "        for j,sent in enumerate(sentences):\n",
    "            merged_seq, segment_id = merge_trim_pad(sent_1=question, sent_2=sent, \n",
    "                                        max_len=max_question_plus_sentence_len, \n",
    "                                        cls_index=cls_index, sep_index=sep_index, pad_index=pad_index)\n",
    "            data_out[\"sequences\"].append(merged_seq)\n",
    "            data_out[\"segment_ids\"].append(segment_id)\n",
    "            data_out[\"supporting_fact\"].append(supp_fact[j])\n",
    "    \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90557/90557 [01:01<00:00, 1479.03it/s]\n"
     ]
    }
   ],
   "source": [
    "data_out = do_a_lot_of_work(questions, flattened_documents, supporting_facts_expanded, \n",
    "                           max_question_plus_sentence_len, cls_index, sep_index, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:sequences, length:3706225\n",
      "Key:segment_ids, length:3706225\n",
      "Key:supporting_fact, length:3706225\n",
      "Key:document_lengths, length:90557\n"
     ]
    }
   ],
   "source": [
    "for key, value in data_out.items():\n",
    "    print(\"Key:{}, length:{}\".format(key, len(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "seq_len = []\n",
    "for seq in data_out[\"sequences\"]:\n",
    "    seq_len.append(len(seq))\n",
    "for seq in data_out[\"segment_ids\"]:\n",
    "    seq_len.append(len(seq))\n",
    "\n",
    "print(min(seq_len))\n",
    "print(max(seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path,out_pkl_name,data_out)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
