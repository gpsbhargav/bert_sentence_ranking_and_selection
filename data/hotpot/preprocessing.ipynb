{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing HotpotQA for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "if(TRAINING):\n",
    "    file_path = \"/home/bhargav/data/hotpotqa/hotpot_train_v1.json\"\n",
    "    out_pkl_name = \"preprocessed_train.pkl\"\n",
    "    problem_indices = [8437, 25197, 34122, 46031, 52955, 63867, 82250]\n",
    "else:\n",
    "    file_path = \"/home/bhargav/data/hotpotqa/hotpot_dev_distractor_v1.json\"\n",
    "    out_pkl_name = \"preprocessed_dev.pkl\"\n",
    "    problem_indices = [5059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "#     exclude = set(string.punctuation)\n",
    "#     clean = ''.join(ch for ch in text if ch not in exclude)\n",
    "#     clean = clean.lower().strip()\n",
    "    text = re.sub(\n",
    "            r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "            str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [03:50<00:00, 32.15it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "paragraphs = [] \n",
    "supporting_facts = []\n",
    "\n",
    "\n",
    "for item_index, item in enumerate(tqdm(dataset)):\n",
    "    if(item_index in problem_indices):\n",
    "        continue\n",
    "    question = tokenize(item[\"question\"])\n",
    "    questions.append(question)\n",
    "    paragraph_names = []\n",
    "    paragraph_text = []\n",
    "    for i,para in enumerate(item[\"context\"]):\n",
    "        para_name = para[0]\n",
    "        para_sents = para[1]\n",
    "        paragraph_names.append(para_name)\n",
    "        paragraph_text.append([tokenize(s) for s in para_sents])\n",
    "    paragraphs.append(paragraph_text)\n",
    "    supp_fact_list = []\n",
    "    for sup_fact in item[\"supporting_facts\"]:\n",
    "        para_name = sup_fact[0]\n",
    "        supporting_fact_index = sup_fact[1] \n",
    "        para_index = paragraph_names.index(para_name)\n",
    "        supp_fact_list.append([para_index, supporting_fact_index])\n",
    "    supporting_facts.append(supp_fact_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020, 3660, 18928, 3385, 1998, 3968, 3536, 1997, 1996, 2168, 10662, 1029]\n"
     ]
    }
   ],
   "source": [
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3968, 3536, 2003, 1037, 2807, 2137, 16747, 2558, 4038, 1011, 3689, 2143, 2856, 1998, 2550, 2011, 5199, 9658, 1010, 1998, 4626, 5206, 2139, 9397, 2004, 8754, 12127, 3968, 3536, 1012], [1996, 2143, 5936, 1996, 2558, 1999, 3536, 1005, 1055, 2166, 2043, 2002, 2081, 2010, 2190, 1011, 2124, 3152, 2004, 2092, 2004, 2010, 3276, 2007, 3364, 20252, 11320, 12333, 2072, 1010, 2209, 2011, 3235, 28570, 1012], [4532, 8201, 6262, 1010, 10717, 12098, 29416, 1010, 10799, 3557, 1010, 7059, 5032, 1010, 1998, 3021, 6264, 2024, 2426, 1996, 4637, 3459, 1012]]\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0], [4, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(supporting_facts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7404\n",
      "7404\n",
      "10\n",
      "7404\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))\n",
    "print(len(paragraphs))\n",
    "print(len(paragraphs[0]))\n",
    "print(len(supporting_facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_paragraph_lengths(document):\n",
    "    lengths = []\n",
    "    for para in document:\n",
    "        lengths.append(len(para))\n",
    "    return lengths, sum(lengths)\n",
    "\n",
    "\n",
    "# returns supporting fact indices so that it can be used later while trimming documents.\n",
    "def expand_supporting_facts(supporting_facts, paragraphs):\n",
    "    supporting_facts_expanded = []\n",
    "    problem_indices = []\n",
    "    supporting_fact_indices = []\n",
    "    for i,supp_facts in enumerate(tqdm(supporting_facts)):\n",
    "        s_f_indices = []\n",
    "        paragraph_lengths, total_num_sentences = compute_paragraph_lengths(paragraphs[i])\n",
    "        s_f_expanded = [0] * total_num_sentences\n",
    "        for para_idx, sentence_idx in supp_facts:\n",
    "            fact_idx = sum(paragraph_lengths[:para_idx])+ sentence_idx\n",
    "            if(fact_idx >= total_num_sentences):\n",
    "                problem_indices.append(i)\n",
    "            else:\n",
    "                s_f_indices.append(fact_idx)\n",
    "        for s_f_idx in s_f_indices:\n",
    "            s_f_expanded[s_f_idx] = 1\n",
    "        supporting_facts_expanded.append(s_f_expanded)\n",
    "        supporting_fact_indices += s_f_indices\n",
    "    return supporting_facts_expanded, set(problem_indices), s_f_indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7404/7404 [00:00<00:00, 105082.20it/s]\n"
     ]
    }
   ],
   "source": [
    "supporting_facts_expanded, problem_indices, supporting_fact_indices = expand_supporting_facts(supporting_facts, paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg supporting fact index:24.0\n",
      "min supporting fact index:17\n",
      "max supporting fact index:31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supporting_fact_indices = np.array(supporting_fact_indices)\n",
    "\n",
    "print(\"Avg supporting fact index:{}\".format(supporting_fact_indices.mean()))\n",
    "print(\"min supporting fact index:{}\".format(supporting_fact_indices.min()))\n",
    "print(\"max supporting fact index:{}\".format(supporting_fact_indices.max()))\n",
    "\n",
    "max_supporting_fact_index = 35\n",
    "np.sum(np.greater(supporting_fact_indices,max_supporting_fact_index))/supporting_fact_indices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(supporting_facts_expanded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_documents(documents):\n",
    "    flattened_documents = []\n",
    "    for doc in tqdm(documents):\n",
    "        f_d = []\n",
    "        for para in doc:\n",
    "            for sent in para:\n",
    "                f_d.append(sent)\n",
    "        flattened_documents.append(f_d)\n",
    "    return flattened_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7404/7404 [00:00<00:00, 84650.73it/s]\n"
     ]
    }
   ],
   "source": [
    "flattened_documents = flatten_documents(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(flattened_documents[0]) == len(supporting_facts_expanded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg document len:41.38614262560778\n",
      "min document len:2\n",
      "max document len:147\n"
     ]
    }
   ],
   "source": [
    "num_sentences_per_document = []\n",
    "\n",
    "for item in supporting_facts_expanded:\n",
    "    num_sentences_per_document.append(len(item))\n",
    "\n",
    "num_sentences_per_document = np.array(num_sentences_per_document)\n",
    "\n",
    "print(\"Avg document len:{}\".format(num_sentences_per_document.mean()))\n",
    "print(\"min document len:{}\".format(num_sentences_per_document.min()))\n",
    "print(\"max document len:{}\".format(num_sentences_per_document.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027552674230145867"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentences_per_document = 65\n",
    "np.sum(np.greater(num_sentences_per_document,max_sentences_per_document))/num_sentences_per_document.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg sentence len:28.85067047839098\n",
      "min sentence len:0\n",
      "max sentence len:453\n"
     ]
    }
   ],
   "source": [
    "sentence_lengths = []\n",
    "\n",
    "for doc in flattened_documents:\n",
    "    for sent in doc:\n",
    "        sentence_lengths.append(len(sent))\n",
    "\n",
    "sentence_lengths = np.array(sentence_lengths)\n",
    "\n",
    "print(\"Avg sentence len:{}\".format(sentence_lengths.mean()))\n",
    "print(\"min sentence len:{}\".format(sentence_lengths.min()))\n",
    "print(\"max sentence len:{}\".format(sentence_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05610871246610078"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_len = 55\n",
    "np.sum(np.greater(sentence_lengths,max_sent_len))/sentence_lengths.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg question len:19.590626688276608\n",
      "min question len:7\n",
      "max question len:65\n"
     ]
    }
   ],
   "source": [
    "question_lengths = np.array([len(q) for q in questions])\n",
    "\n",
    "print(\"Avg question len:{}\".format(question_lengths.mean()))\n",
    "print(\"min question len:{}\".format(question_lengths.min()))\n",
    "print(\"max question len:{}\".format(question_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0035116153430578066"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len = 45\n",
    "np.sum(np.greater(question_lengths,max_question_len))/question_lengths.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_plus_sentence_len = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_index = tokenizer.convert_tokens_to_ids([\"[CLS]\"])[0]\n",
    "sep_index = tokenizer.convert_tokens_to_ids([\"[SEP]\"])[0]\n",
    "pad_index = tokenizer.convert_tokens_to_ids([\"[PAD]\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_trim_pad(sent_1, sent_2, max_len, cls_index, sep_index, pad_index):\n",
    "    merged_seq = [cls_index] + sent_1 + [sep_index] + sent_2\n",
    "    merged_seq = merged_seq[:max_len-1]\n",
    "    merged_seq.append(sep_index)\n",
    "    merged_seq = merged_seq + [pad_index] * (max_len - len(merged_seq))\n",
    "    num_zeros = merged_seq.index(sep_index) + 1 # index of [SEP] + 1 =  number of zeros to add\n",
    "    segment_id = [0]*num_zeros + [1]*(len(merged_seq)-num_zeros)\n",
    "    return merged_seq, segment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a messy function. I have wrapped it in a function because i have lifted this code from my squad sentence selector\n",
    "# and the variable names inside and outside the functions conflict.\n",
    "def do_a_lot_of_work(questions_tokenized, sentences_tokenized, supporting_facts, max_sentences_per_passage, \n",
    "                     max_question_plus_sentence_len, cls_index, sep_index, pad_index):\n",
    "    # init data dict\n",
    "    data_out = {}\n",
    "\n",
    "    for i in range(max_sentences_per_passage):\n",
    "        data_out[\"sequence_{}\".format(i)] = []\n",
    "        data_out[\"sequence_segment_id_{}\".format(i)] = []\n",
    "\n",
    "    data_out[\"passage_mask\"] = []\n",
    "    data_out[\"supporting_fact\"] = []\n",
    "\n",
    "\n",
    "    for i in trange(len(questions_tokenized)):\n",
    "        question = questions_tokenized[i]\n",
    "        sentences = sentences_tokenized[i][:max_sentences_per_passage]\n",
    "        num_pad_sentences = max_sentences_per_passage - len(sentences)\n",
    "        sentences = sentences + [[]]*(num_pad_sentences)\n",
    "\n",
    "        passage_mask = [1] * (max_sentences_per_passage-num_pad_sentences) + [0]*num_pad_sentences\n",
    "        data_out[\"passage_mask\"].append(passage_mask)\n",
    "        \n",
    "        supp_fact = supporting_facts[i]\n",
    "        supp_fact = supp_fact[:max_sentences_per_passage]\n",
    "        supp_fact = supp_fact + [0]*(num_pad_sentences)\n",
    "        data_out[\"supporting_fact\"].append(supp_fact)\n",
    "\n",
    "        for j,sent in enumerate(sentences):\n",
    "            merged_seq, segment_id = merge_trim_pad(sent_1=question, sent_2=sent, \n",
    "                                        max_len=max_question_plus_sentence_len, \n",
    "                                        cls_index=cls_index, sep_index=sep_index, pad_index=pad_index)\n",
    "            data_out[\"sequence_{}\".format(j)].append(merged_seq)\n",
    "            data_out[\"sequence_segment_id_{}\".format(j)].append(segment_id)\n",
    "\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7404/7404 [00:08<00:00, 826.14it/s]\n"
     ]
    }
   ],
   "source": [
    "data_out = do_a_lot_of_work(questions, flattened_documents, supporting_facts_expanded, \n",
    "                           max_sentences_per_document, max_question_plus_sentence_len, cls_index, sep_index, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sequence_0', 'sequence_segment_id_0', 'sequence_1', 'sequence_segment_id_1', 'sequence_2', 'sequence_segment_id_2', 'sequence_3', 'sequence_segment_id_3', 'sequence_4', 'sequence_segment_id_4', 'sequence_5', 'sequence_segment_id_5', 'sequence_6', 'sequence_segment_id_6', 'sequence_7', 'sequence_segment_id_7', 'sequence_8', 'sequence_segment_id_8', 'sequence_9', 'sequence_segment_id_9', 'sequence_10', 'sequence_segment_id_10', 'sequence_11', 'sequence_segment_id_11', 'sequence_12', 'sequence_segment_id_12', 'sequence_13', 'sequence_segment_id_13', 'sequence_14', 'sequence_segment_id_14', 'sequence_15', 'sequence_segment_id_15', 'sequence_16', 'sequence_segment_id_16', 'sequence_17', 'sequence_segment_id_17', 'sequence_18', 'sequence_segment_id_18', 'sequence_19', 'sequence_segment_id_19', 'sequence_20', 'sequence_segment_id_20', 'sequence_21', 'sequence_segment_id_21', 'sequence_22', 'sequence_segment_id_22', 'sequence_23', 'sequence_segment_id_23', 'sequence_24', 'sequence_segment_id_24', 'sequence_25', 'sequence_segment_id_25', 'sequence_26', 'sequence_segment_id_26', 'sequence_27', 'sequence_segment_id_27', 'sequence_28', 'sequence_segment_id_28', 'sequence_29', 'sequence_segment_id_29', 'sequence_30', 'sequence_segment_id_30', 'sequence_31', 'sequence_segment_id_31', 'sequence_32', 'sequence_segment_id_32', 'sequence_33', 'sequence_segment_id_33', 'sequence_34', 'sequence_segment_id_34', 'sequence_35', 'sequence_segment_id_35', 'sequence_36', 'sequence_segment_id_36', 'sequence_37', 'sequence_segment_id_37', 'sequence_38', 'sequence_segment_id_38', 'sequence_39', 'sequence_segment_id_39', 'sequence_40', 'sequence_segment_id_40', 'sequence_41', 'sequence_segment_id_41', 'sequence_42', 'sequence_segment_id_42', 'sequence_43', 'sequence_segment_id_43', 'sequence_44', 'sequence_segment_id_44', 'sequence_45', 'sequence_segment_id_45', 'sequence_46', 'sequence_segment_id_46', 'sequence_47', 'sequence_segment_id_47', 'sequence_48', 'sequence_segment_id_48', 'sequence_49', 'sequence_segment_id_49', 'sequence_50', 'sequence_segment_id_50', 'sequence_51', 'sequence_segment_id_51', 'sequence_52', 'sequence_segment_id_52', 'sequence_53', 'sequence_segment_id_53', 'sequence_54', 'sequence_segment_id_54', 'sequence_55', 'sequence_segment_id_55', 'sequence_56', 'sequence_segment_id_56', 'sequence_57', 'sequence_segment_id_57', 'sequence_58', 'sequence_segment_id_58', 'sequence_59', 'sequence_segment_id_59', 'sequence_60', 'sequence_segment_id_60', 'sequence_61', 'sequence_segment_id_61', 'sequence_62', 'sequence_segment_id_62', 'sequence_63', 'sequence_segment_id_63', 'sequence_64', 'sequence_segment_id_64', 'passage_mask', 'supporting_fact'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7404\n",
      "7404\n"
     ]
    }
   ],
   "source": [
    "num_sequences_in_each_position = []\n",
    "for i in range(max_sentences_per_document):\n",
    "    num_sequences_in_each_position.append(len(data_out[\"sequence_{}\".format(i)]))\n",
    "    num_sequences_in_each_position.append(len(data_out[\"sequence_segment_id_{}\".format(i)]))\n",
    "\n",
    "num_sequences_in_each_position.append(len(data_out[\"passage_mask\"]))\n",
    "num_sequences_in_each_position.append(len(data_out[\"supporting_fact\"]))\n",
    "    \n",
    "print(min(num_sequences_in_each_position))\n",
    "print(max(num_sequences_in_each_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "all_lengths = []\n",
    "\n",
    "for i in range(max_sentences_per_document):\n",
    "#     for item in data_out[\"sequence_{}\".format(i)]:\n",
    "#         all_lengths.append(len(item))\n",
    "    for item in data_out[\"sequence_segment_id_{}\".format(i)]:\n",
    "        all_lengths.append(len(item))\n",
    "    \n",
    "print(min(all_lengths))\n",
    "print(max(all_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({103: 481260})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(all_lengths)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "passage_mask_and_sf_lengths = []\n",
    "\n",
    "for item in data_out[\"passage_mask\"]:\n",
    "    passage_mask_and_sf_lengths.append(len(item))\n",
    "    \n",
    "for item in data_out[\"supporting_fact\"]:\n",
    "    passage_mask_and_sf_lengths.append(len(item))\n",
    "    \n",
    "print(min(passage_mask_and_sf_lengths))\n",
    "print(max(passage_mask_and_sf_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path,out_pkl_name,data_out)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
