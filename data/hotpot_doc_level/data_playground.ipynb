{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess HotpotQA for BERT\n",
    "Notebook output: (question,context) pairs. Model should predict which sentences in the context are required to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "if(TRAINING):\n",
    "    file_path = \"/home/bhargav/data/hotpotqa/hotpot_train_v1.json\"\n",
    "    out_pkl_name = \"preprocessed_train.pkl\"\n",
    "    problem_indices = [8437, 25197, 34122, 46031, 52955, 63867, 82250]\n",
    "else:\n",
    "    file_path = \"/home/bhargav/data/hotpotqa/hotpot_dev_distractor_v1.json\"\n",
    "    out_pkl_name = \"preprocessed_dev.pkl\"\n",
    "    problem_indices = [5059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [03:52<00:00, 31.82it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "paragraphs = [] \n",
    "supporting_facts = []\n",
    "\n",
    "\n",
    "for item_index, item in enumerate(tqdm(dataset)):\n",
    "    if(item_index in problem_indices):\n",
    "        continue\n",
    "    question = tokenize(item[\"question\"])\n",
    "    questions.append(question)\n",
    "    paragraph_names = []\n",
    "    paragraph_text = []\n",
    "    for i,para in enumerate(item[\"context\"]):\n",
    "        para_name = para[0]\n",
    "        para_sents = para[1]\n",
    "        paragraph_names.append(para_name)\n",
    "        paragraph_text.append([tokenize(s) for s in para_sents])\n",
    "    paragraphs.append(paragraph_text)\n",
    "    supp_fact_list = []\n",
    "    for sup_fact in item[\"supporting_facts\"]:\n",
    "        para_name = sup_fact[0]\n",
    "        supporting_fact_index = sup_fact[1] \n",
    "        para_index = paragraph_names.index(para_name)\n",
    "        supp_fact_list.append([para_index, supporting_fact_index])\n",
    "    supporting_facts.append(supp_fact_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max index of supporting fact: 11\n",
      "Avg index of supporting fact: 0.7631798233431476\n"
     ]
    }
   ],
   "source": [
    "indices_of_supporting_facts = []\n",
    "for datapoint in supporting_facts:\n",
    "    for s_f in datapoint:\n",
    "        indices_of_supporting_facts.append(s_f[1])\n",
    "        \n",
    "indices_of_supporting_facts = np.array(indices_of_supporting_facts)\n",
    "\n",
    "print(\"Max index of supporting fact: {}\".format(indices_of_supporting_facts.max()))\n",
    "print(\"Avg index of supporting fact: {}\".format(indices_of_supporting_facts.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005721904338647853"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_s_f_index = 5\n",
    "np.sum(np.greater(indices_of_supporting_facts,max_s_f_index))/indices_of_supporting_facts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg question len:19.590626688276608\n",
      "min question len:7\n",
      "max question len:65\n"
     ]
    }
   ],
   "source": [
    "question_lengths = np.array([len(q) for q in questions])\n",
    "\n",
    "print(\"Avg question len:{}\".format(question_lengths.mean()))\n",
    "print(\"min question len:{}\".format(question_lengths.min()))\n",
    "print(\"max question len:{}\".format(question_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg document len:1194.0179632631011\n",
      "min document len:66\n",
      "max document len:3222\n"
     ]
    }
   ],
   "source": [
    "document_lengths = []\n",
    "\n",
    "for doc in paragraphs:\n",
    "    doc_len = 0\n",
    "    for para in doc:\n",
    "        for sent in para:\n",
    "            doc_len += len(sent)\n",
    "    document_lengths.append(doc_len)\n",
    "\n",
    "document_lengths = np.array(document_lengths)\n",
    "\n",
    "\n",
    "print(\"Avg document len:{}\".format(document_lengths.mean()))\n",
    "print(\"min document len:{}\".format(document_lengths.min()))\n",
    "print(\"max document len:{}\".format(document_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg combined len:1213.6085899513776\n",
      "min combined len:80\n",
      "max combined len:3231\n"
     ]
    }
   ],
   "source": [
    "question_doc_combined_len = question_lengths + document_lengths\n",
    "\n",
    "print(\"Avg combined len:{}\".format(question_doc_combined_len.mean()))\n",
    "print(\"min combined len:{}\".format(question_doc_combined_len.min()))\n",
    "print(\"max combined len:{}\".format(question_doc_combined_len.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg supporting facts len:159.5958941112912\n",
      "min supporting facts len:32\n",
      "max supporting facts len:666\n"
     ]
    }
   ],
   "source": [
    "supporting_fact_lengths = []\n",
    "\n",
    "for i,doc in enumerate(paragraphs):\n",
    "    supp_fact_len = 0\n",
    "    for j, para in enumerate(doc):\n",
    "        for k, sent in enumerate(para):\n",
    "            if([j,k] in supporting_facts[i]):\n",
    "                supp_fact_len += len(sent)\n",
    "    supporting_fact_lengths.append(supp_fact_len)\n",
    "    \n",
    "supporting_fact_lengths +=  np.array(supporting_fact_lengths)\n",
    "\n",
    "print(\"Avg supporting facts len:{}\".format(supporting_fact_lengths.mean()))\n",
    "print(\"min supporting facts len:{}\".format(supporting_fact_lengths.min()))\n",
    "print(\"max supporting facts len:{}\".format(supporting_fact_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg question_plus_supp_fact_onlys len:179.1865207995678\n",
      "min question_plus_supp_fact_only len:44\n",
      "max question_plus_supp_fact_only len:707\n"
     ]
    }
   ],
   "source": [
    "question_plus_supp_fact_only = question_lengths + supporting_fact_lengths\n",
    "\n",
    "print(\"Avg question_plus_supp_fact_onlys len:{}\".format(question_plus_supp_fact_only.mean()))\n",
    "print(\"min question_plus_supp_fact_only len:{}\".format(question_plus_supp_fact_only.min()))\n",
    "print(\"max question_plus_supp_fact_only len:{}\".format(question_plus_supp_fact_only.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg question_plus_gold_paragraphs_lengths len:415.1766612641815\n",
      "min question_plus_gold_paragraphs_lengths len:100\n",
      "max question_plus_gold_paragraphs_lengths len:1356\n"
     ]
    }
   ],
   "source": [
    "question_plus_gold_paragraphs_lengths = []\n",
    "\n",
    "for i,doc in enumerate(paragraphs):\n",
    "    supp_sentences_len = question_lengths[i]\n",
    "    for j, para in enumerate(doc):\n",
    "        if(j in [f[0] for f in supporting_facts[i]]):\n",
    "            for k, sent in enumerate(para):\n",
    "                supp_sentences_len += len(sent)\n",
    "    question_plus_gold_paragraphs_lengths.append(supp_sentences_len)\n",
    "    \n",
    "question_plus_gold_paragraphs_lengths +=  np.array(question_plus_gold_paragraphs_lengths)\n",
    "\n",
    "print(\"Avg question_plus_gold_paragraphs_lengths len:{}\".format(question_plus_gold_paragraphs_lengths.mean()))\n",
    "print(\"min question_plus_gold_paragraphs_lengths len:{}\".format(question_plus_gold_paragraphs_lengths.min()))\n",
    "print(\"max question_plus_gold_paragraphs_lengths len:{}\".format(question_plus_gold_paragraphs_lengths.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2516207455429498"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 500\n",
    "np.sum(np.greater(question_plus_gold_paragraphs_lengths,max_len))/question_plus_gold_paragraphs_lengths.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05172879524581307"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 300\n",
    "np.sum(np.greater(question_plus_supp_fact_only,max_len))/question_plus_supp_fact_only.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg num_sentences_per_para:4.1582711358393265\n",
      "min num_sentences_per_para:1\n",
      "max num_sentences_per_para:85\n"
     ]
    }
   ],
   "source": [
    "num_sentences_per_para = []\n",
    "\n",
    "for doc in paragraphs:\n",
    "    for para in doc:\n",
    "        num_sentences_per_para.append(len(para))\n",
    "\n",
    "num_sentences_per_para = np.array(num_sentences_per_para)\n",
    "\n",
    "\n",
    "print(\"Avg num_sentences_per_para:{}\".format(num_sentences_per_para.mean()))\n",
    "print(\"min num_sentences_per_para:{}\".format(num_sentences_per_para.min()))\n",
    "print(\"max num_sentences_per_para:{}\".format(num_sentences_per_para.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07671325824399511"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 7\n",
    "np.sum(np.greater(num_sentences_per_para,max_len))/num_sentences_per_para.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg question_plus_para_len:279.1744605780974\n",
      "min question_plus_para_len:42\n",
      "max question_plus_para_len:3742\n"
     ]
    }
   ],
   "source": [
    "question_plus_para_len = []\n",
    "\n",
    "for i,doc in enumerate(paragraphs):\n",
    "    for j, para in enumerate(doc):\n",
    "        sentences_len = question_lengths[i]\n",
    "        for k, sent in enumerate(para):\n",
    "            sentences_len += len(sent)\n",
    "        question_plus_para_len.append(sentences_len)\n",
    "    \n",
    "question_plus_para_len +=  np.array(question_plus_para_len)\n",
    "\n",
    "print(\"Avg question_plus_para_len:{}\".format(question_plus_para_len.mean()))\n",
    "print(\"min question_plus_para_len:{}\".format(question_plus_para_len.min()))\n",
    "print(\"max question_plus_para_len:{}\".format(question_plus_para_len.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06299362192970552"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 500\n",
    "np.sum(np.greater(question_plus_para_len,max_len))/question_plus_para_len.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
